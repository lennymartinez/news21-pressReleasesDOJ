{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping DoJ Press releases\n",
    "\n",
    "In summer 2018, I downloaded press releases from the Department of Justice and then worked to process them for the News21 Hate In America investigation. My goal is to recreate the process, but more cleanly. Since I originally scraped this, I've learned more about Python and about scraping and so I want to incorporate that learning into this project. For now, I've moved the 2018 work to it's own separate folder.\n",
    "\n",
    "The process for redoing this total analysis is:\n",
    "\n",
    "1. Get the URL to all the press releases since 2009\n",
    "1. Get the text from the press releases and save them individually\n",
    "1. Identify which subset of all the press releases relate to hate crimes\n",
    "1. Isolate this subset.\n",
    "1. Within this subset, figure out all the people mentioned. \n",
    "1. Use the list of people to identify cases related to the 2019 hate crimes prevention act.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Get the URL to all the press releases since 2019\n",
    "\n",
    "Our first step is to get all the press releases from: [https://www.justice.gov/news](https://www.justice.gov/news)\n",
    "\n",
    "Let's import the libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# used to get make the http requests (at least I think they're http requests)\n",
    "import requests\n",
    "\n",
    "# used to keep track of when the data was scraped\n",
    "from datetime import datetime\n",
    "\n",
    "# import urllib.parse as parse\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# used to add pauses to the code to make sure the requests and processing happen smoothly.\n",
    "from time import sleep\n",
    "\n",
    "# used to turn the lists into csv files for backup\n",
    "import csv\n",
    "\n",
    "# use the os library to make sure we have the folder where the scraped materials will go\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's set up the baseurl, and the payload which contains the extra parameters we'll add to the query to make sure we get the press releases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseurl = \"https://www.justice.gov/news\"\n",
    "\n",
    "payload = {\"items_per_page\":50,\n",
    "        \"f[0]\":\"type:press_release\",\n",
    "        \"page\": \"0\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As more press releases are added, the total number of pages increases. When I initially scrapped this, Summer 2018, there were about 255 pages to scrape. I made the `max_page` variable to keep track of the changing number.\n",
    "\n",
    "To check what the number should be go to [this link](https://www.justice.gov/news?items_per_page=50&f%5B0%5D=type%3Apress_release) and scroll to the bottom and click on \"Last\". Then check the url for the page number, which should be the last part of the url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_page = 292"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we loop through all pages to get the URLs to each page of press releases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "today = datetime.now()\n",
    "\n",
    "if int(today.month) < 10: \n",
    "    scrape_date = str(today.year)+\"-0\"+str(today.month)+\"-\"+str(today.day)\n",
    "else:\n",
    "    scrape_date = str(today.year)+\"-\"+str(today.month)+\"-\"+str(today.day)\n",
    "\n",
    "url_list = [[\"scrape_date\", \"url\"]]\n",
    "\n",
    "# for i in range(0, max_page):\n",
    "    \n",
    "#     # set the current \n",
    "#     payload[\"page\"] = str(i)\n",
    "    \n",
    "#     # make the request\n",
    "#     response = requests.get(baseurl, params=payload)\n",
    "    \n",
    "#     # add url to the url_list\n",
    "#     url_list.append([scrape_date, response.url])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
